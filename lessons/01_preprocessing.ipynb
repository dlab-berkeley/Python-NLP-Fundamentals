{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# An√°lisis de texto en Python: Preprocesamiento\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "\n",
    "* Aprender los pasos comunes para el preprocesamiento de datos de texto, as√≠ como las operaciones espec√≠ficas para el preprocesamiento de datos de Twitter.\n",
    "* Conocer los paquetes de PLN m√°s utilizados y sus capacidades.\n",
    "* Comprender los tokenizadores y c√≥mo han cambiado desde la llegada de los Modelos de Lenguaje Grandes.\n",
    "</div>\n",
    "\n",
    "### Iconos utilizados en este cuaderno\n",
    "üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender qu√© est√° pasando.\n",
    "ü•ä **Desaf√≠o**: Ejercicios interactivos. ¬°Los trabajaremos en el taller!<br>\n",
    "‚ö†Ô∏è **Advertencia:** Tenga cuidado con cosas complicadas o errores comunes.<br>\n",
    "üé¨ **Demostraci√≥n**: Mostrando algo m√°s avanzado, ¬°para que sepas para qu√© se puede usar Python!<br>\n",
    "\n",
    "### Secciones\n",
    "1. [Preprocesamiento](#section1)\n",
    "2. [Tokenizacion](#section2)\n",
    "\n",
    "En esta serie de talleres de tres partes, aprenderemos los fundamentos para realizar an√°lisis de texto en Python. Estas t√©cnicas se enmarcan en el √°mbito del Procesamiento del Lenguaje Natural (PLN). El PLN es un campo que se ocupa de la identificaci√≥n y extracci√≥n de patrones del lenguaje, principalmente en textos escritos. A lo largo de la serie de talleres, interactuaremos con diversos paquetes para realizar an√°lisis de texto: desde m√©todos de cadenas simples hasta paquetes espec√≠ficos de PLN, como `nltk`, `spaCy` y los m√°s recientes sobre Modelos de Lenguaje Grandes (`BERT`).\n",
    "\n",
    "Ahora, instalemos estos paquetes correctamente antes de profundizar en el material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442e4c7-e926-493d-a64e-516616ad915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install packages/model\n",
    "# %pip install NLTK\n",
    "# %pip install transformers\n",
    "# %pip install spaCy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b8f8e-4e69-426e-a202-ec48b325e89a",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Preprocesamiento\n",
    "\n",
    "En la Parte 1 de este taller, abordaremos el primer paso del an√°lisis de texto. Nuestro objetivo es convertir los datos de texto sin procesar y desordenados a un formato consistente. Este proceso se conoce como **preprocesamiento**, **limpieza de texto** o **normalizaci√≥n de texto**.\n",
    "\n",
    "Observar√° que, al finalizar el preprocesamiento, nuestros datos a√∫n se encuentran en un formato legible y comprensible. En las Partes 2 y 3, comenzaremos a explorar la conversi√≥n de los datos de texto a una representaci√≥n num√©rica, un formato que las computadoras pueden procesar con mayor facilidad.\n",
    "\n",
    "üîî **Pregunta**: Deteng√°monos un momento para reflexionar sobre **sus** experiencias previas trabajando con datos de texto.\n",
    "- ¬øCu√°l es el formato de los datos de texto con los que han interactuado (texto plano, CSV o XML)?\n",
    "- ¬øDe d√≥nde provienen (corpus estructurado, datos extra√≠dos de la web, datos de encuestas)?\n",
    "- ¬øEst√°n desordenados (es decir, tienen un formato uniforme)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35911a-3b3f-4a48-a7d1-9882aab04851",
   "metadata": {},
   "source": [
    "## Procesos Comunes\n",
    "\n",
    "El preprocesamiento no se logra con una sola l√≠nea de c√≥digo. A menudo, comenzamos familiariz√°ndonos con los datos y, a medida que avanzamos, obtenemos una comprensi√≥n m√°s clara de la granularidad del preprocesamiento que queremos aplicar.\n",
    "\n",
    "Normalmente, empezamos aplicando un conjunto de procesos comunes para limpiar los datos. Estas operaciones no alteran sustancialmente la forma ni el significado de los datos; sirven como un procedimiento estandarizado para remodelarlos en un formato consistente.\n",
    "\n",
    "Los siguientes procesos, por ejemplo, se aplican com√∫nmente para preprocesar textos en ingl√©s de diversos g√©neros. Estas operaciones se pueden realizar mediante funciones integradas de Python, como los m√©todos `string` y las expresiones regulares.\n",
    "\n",
    "- Min√∫sculas\n",
    "- Eliminar signos de puntuaci√≥n\n",
    "- Eliminar espacios en blanco adicionales\n",
    "- Eliminar palabras vac√≠as\n",
    "\n",
    "Tras el procesamiento inicial, podemos optar por realizar procesos espec√≠ficos para cada tarea, cuyos detalles suelen depender de la tarea posterior que queramos realizar y de la naturaleza de los datos textuales (es decir, sus caracter√≠sticas estil√≠sticas y ling√º√≠sticas). Antes de profundizar en estas operaciones, ¬°echemos un vistazo a nuestros datos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d7350-9a1e-4db9-b828-a87fe1676d8d",
   "metadata": {},
   "source": [
    "### Importar los datos de texto\n",
    "\n",
    "Los datos de texto con los que trabajaremos son un archivo CSV. Contiene tuits sobre aerol√≠neas estadounidenses, recopilados desde febrero de 2015.\n",
    "\n",
    "Leamos el archivo `airline_tweets.csv` en un marco de datos con `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ff64b-53ad-4eca-b846-3fda20085c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la librer√≠a pandas\n",
    "import pandas as pd\n",
    "\n",
    "# se define la ruta del archivo CSV que contiene los datos, ../ significa ‚Äúsube un nivel de carpeta‚Äù (va a la carpeta padre).\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "\n",
    "# Usa Pandas (pd) para leer el archivo CSV y guardarlo en un DataFrame llamado tweets.\n",
    "# sep=',' indica que el separador de las columnas en el CSV es la coma (,).\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397ac6a-c2ba-4cce-8700-b36b38026c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Muestra las primeras 5 filas del DataFrame (por defecto son 5, pero se puede pasar otro n√∫mero: tweets.head(10))\n",
    "# Sirve para hacer una vista previa r√°pida de los datos cargados.\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b339f-45cf-465d-931c-05f9096fd510",
   "metadata": {},
   "source": [
    "El dataframe tiene una fila por tuit. El texto del tuit se muestra en la columna `text`.\n",
    "- `text` (`str`): el texto del tuit.\n",
    "\n",
    "Otros metadatos de inter√©s son:\n",
    "- `airline_sentiment` (`str`): el sentimiento del tuit, etiquetado como \"neutral\", \"positivo\" o \"negativo\".\n",
    "- `airline` (`str`): la aerol√≠nea sobre la que se tuitea.\n",
    "- `retweet count` (`int`): cu√°ntas veces se retuite√≥ el tuit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c695b-4bd1-4151-9cb9-ef5253eb16df",
   "metadata": {},
   "source": [
    "Echemos un vistazo a algunos de los tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690daab-7be5-4b8f-8af0-a91fdec4ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n"
     ]
    }
   ],
   "source": [
    "# Accede a la columna text del DataFrame (donde est√°n los tweets), he imprime los tweets por orden.\n",
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc05fa-ad30-4402-ab56-086bcb09a166",
   "metadata": {},
   "source": [
    "üîî **Pregunta**: ¬øQu√© has notado? ¬øCu√°les son las caracter√≠sticas estil√≠sticas de los tuits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3460393-00a6-461c-b02a-9e98f9b5d1af",
   "metadata": {},
   "source": [
    "### Min√∫sculas\n",
    "\n",
    "Si bien reconocemos que el uso de may√∫sculas y min√∫sculas en una palabra es informativo, a menudo no trabajamos en contextos donde podamos utilizar esta informaci√≥n correctamente.\n",
    "\n",
    "Con frecuencia, el an√°lisis posterior que realizamos **no distingue entre may√∫sculas y min√∫sculas**. Por ejemplo, en el an√°lisis de frecuencia, queremos tener en cuenta las diversas formas de una misma palabra. Convertir los datos de texto en min√∫sculas facilita este proceso y simplifica nuestro an√°lisis.\n",
    "\n",
    "Podemos convertir f√°cilmente en min√∫sculas con el m√©todo de cadena [`.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower); consulte la [documentaci√≥n](https://docs.python.org/3/library/stdtypes.html#string-methods) para obtener m√°s funciones √∫tiles.\n",
    "\n",
    "Apliqu√©moslo al siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a95d90-3ef1-4bff-9cfe-d447ed99f252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica I was scheduled for SFO 2 DAL flight 714 today. Changed to 24th due weather. Looks like flight still on?\n"
     ]
    }
   ],
   "source": [
    "# Selecciona el tweet que est√° en la fila n√∫mero 108, lo guarda en la variable \"first_example\"\n",
    "# y lo imprime.\n",
    "first_example = tweets['text'][108]\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d91c0-6eed-4591-95fc-cd2eae2e0d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "==================================================\n",
      "@virginamerica i was scheduled for sfo 2 dal flight 714 today. changed to 24th due weather. looks like flight still on?\n",
      "==================================================\n",
      "@VIRGINAMERICA I WAS SCHEDULED FOR SFO 2 DAL FLIGHT 714 TODAY. CHANGED TO 24TH DUE WEATHER. LOOKS LIKE FLIGHT STILL ON?\n"
     ]
    }
   ],
   "source": [
    "# Verifica si todo el texto de \"first_example\" est√° en min√∫scula.\n",
    "print(first_example.islower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Imprime una l√≠nea separadora de 50 signos = para que el resultado sea m√°s legible en la consola.\n",
    "print(first_example.lower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Convierte todo el texto del tweet a min√∫sculas.\n",
    "print(first_example.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d",
   "metadata": {},
   "source": [
    "### Eliminar espacios en blanco adicionales\n",
    "\n",
    "A veces nos encontramos con textos con espacios en blanco innecesarios, como espacios, tabulaciones y caracteres de nueva l√≠nea, lo cual es particularmente com√∫n cuando el texto se extrae de p√°ginas web. Antes de profundizar en los detalles, presentemos brevemente las expresiones regulares (regex) y el paquete `re`.\n",
    "\n",
    "Las expresiones regulares son una forma eficaz de buscar patrones de cadenas espec√≠ficos en corpus extensos. Su curva de aprendizaje es notablemente pronunciada, pero pueden ser muy eficientes una vez que las dominamos. Muchos paquetes de PLN dependen en gran medida de las expresiones regulares. Los evaluadores de expresiones regulares, como [regex101](https://regex101.com), son herramientas √∫tiles tanto para comprender como para crear expresiones regulares.\n",
    "\n",
    "Nuestro objetivo en este taller no es ofrecer una introducci√≥n profunda (ni siquiera superficial) a las expresiones regulares; en cambio, queremos presentarles para que est√©n mejor preparados para profundizar en el futuro.\n",
    "\n",
    "El siguiente ejemplo es un poema de William Wordsworth. Como muchos poemas, el texto puede contener saltos de l√≠nea adicionales (es decir, caracteres de nueva l√≠nea, `\\n`) que queremos eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd73f1-a30f-4269-a05e-47cfff7b496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqu√≠ se define la ruta del archivo que contiene el poema.\n",
    "text_path = '../data/poem_wordsworth.txt'\n",
    "\n",
    "# Abre el archivo en modo lectura, Se usa with porque garantiza que el archivo se cierre autom√°ticamente al final, aunque ocurra un error.\n",
    "#Lee todo el contenido del archivo y lo guarda en la variable text como un string.\n",
    "with open(text_path, 'r') as file:\n",
    "    text = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a693dd9-9706-40b3-863f-f568020245f7",
   "metadata": {},
   "source": [
    "Como puedes ver, el poema est√° formateado como una cadena continua de texto con saltos de l√≠nea al final de cada l√≠nea, lo que dificulta su lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e78a75a-8e15-4bcb-a416-783aa7f60ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wandered lonely as a cloud\\n\\n\\nI wandered lonely as a cloud\\nThat floats on high o'er vales and hills,\\nWhen all at once I saw a crowd,\\nA host, of golden daffodils;\\nBeside the lake, beneath the trees,\\nFluttering and dancing in the breeze.\\n\\nContinuous as the stars that shine\\nAnd twinkle on the milky way,\\nThey stretched in never-ending line\\nAlong the margin of a bay:\\nTen thousand saw I at a glance,\\nTossing their heads in sprightly dance.\\n\\nThe waves beside them danced; but they\\nOut-did the sparkling waves in glee:\\nA poet could not but be gay,\\nIn such a jocund company:\\nI gazed‚Äîand gazed‚Äîbut little thought\\nWhat wealth the show to me had brought:\\n\\nFor oft, when on my couch I lie\\nIn vacant or in pensive mood,\\nThey flash upon that inward eye\\nWhich is the bliss of solitude;\\nAnd then my heart with pleasure fills,\\nAnd dances with the daffodils.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cce993-c315-4aaa-87fe-149de8607f65",
   "metadata": {},
   "source": [
    "Una funci√≥n √∫til para mostrar el poema correctamente es `.splitlines()`. Como su nombre indica, divide una secuencia de texto larga en una lista de l√≠neas cuando hay un car√°cter de nueva l√≠nea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I wandered lonely as a cloud',\n",
       " '',\n",
       " '',\n",
       " 'I wandered lonely as a cloud',\n",
       " \"That floats on high o'er vales and hills,\",\n",
       " 'When all at once I saw a crowd,',\n",
       " 'A host, of golden daffodils;',\n",
       " 'Beside the lake, beneath the trees,',\n",
       " 'Fluttering and dancing in the breeze.',\n",
       " '',\n",
       " 'Continuous as the stars that shine',\n",
       " 'And twinkle on the milky way,',\n",
       " 'They stretched in never-ending line',\n",
       " 'Along the margin of a bay:',\n",
       " 'Ten thousand saw I at a glance,',\n",
       " 'Tossing their heads in sprightly dance.',\n",
       " '',\n",
       " 'The waves beside them danced; but they',\n",
       " 'Out-did the sparkling waves in glee:',\n",
       " 'A poet could not but be gay,',\n",
       " 'In such a jocund company:',\n",
       " 'I gazed‚Äîand gazed‚Äîbut little thought',\n",
       " 'What wealth the show to me had brought:',\n",
       " '',\n",
       " 'For oft, when on my couch I lie',\n",
       " 'In vacant or in pensive mood,',\n",
       " 'They flash upon that inward eye',\n",
       " 'Which is the bliss of solitude;',\n",
       " 'And then my heart with pleasure fills,',\n",
       " 'And dances with the daffodils.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide el contenido del poema (text) en l√≠neas y devuelve una lista.\n",
    "text.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3825b-0857-44e1-bf6a-d8c7a9032704",
   "metadata": {},
   "source": [
    "Volvamos a nuestros datos de tweets para ver un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a81ea9-65c4-474a-8530-35393555d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecciona el tweet n√∫mero 5 de la columna text en el DataFrame tweets y lo guarda en la variable second_example y muestra el contenido.\n",
    "second_example = tweets['text'][5]\n",
    "second_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef55865-36fd-4c06-a765-530cf3b53096",
   "metadata": {},
   "source": [
    "En este caso, no queremos dividir el tuit en una lista de cadenas. Seguimos esperando una sola cadena de texto, pero queremos eliminar por completo el salto de l√≠nea.\n",
    "\n",
    "El m√©todo de cadena `.strip()` elimina eficazmente los espacios en ambos extremos del texto. Sin embargo, no funcionar√° en nuestro ejemplo, ya que el car√°cter de nueva l√≠nea est√° en medio de la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933503b-4370-4dc4-b287-6dc2f9cdb1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El m√©todo .strip() elimina espacios en blanco al inicio y al final del tweet.\n",
    "second_example.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b80b4-804f-460f-a2d5-adbd654902b3",
   "metadata": {},
   "source": [
    "Aqu√≠ es donde las expresiones regulares pueden ser realmente √∫tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceac9714-7053-4b2e-affb-71f8c3d2dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f08d20-ba81-4e48-9e2a-5728148005b3",
   "metadata": {},
   "source": [
    "Ahora, con expresiones regulares, b√°sicamente las llamamos para que coincidan con un patr√≥n identificado en los datos de texto, y queremos realizar algunas operaciones con la parte coincidente: extraerla, reemplazarla o eliminarla por completo. Por lo tanto, el funcionamiento de las expresiones regulares se puede resumir en los siguientes pasos:\n",
    "\n",
    "- Identificar y escribir el patr√≥n en la expresi√≥n regular (`r'PATTERN'`)\n",
    "- Escribir el reemplazo para el patr√≥n (`'REPLACEMENT'`)\n",
    "- Llamar a la funci√≥n espec√≠fica de la expresi√≥n regular (p. ej., `re.sub()`)\n",
    "\n",
    "En nuestro ejemplo, el patr√≥n que buscamos es `\\s`, que es la abreviatura de la expresi√≥n regular para cualquier espacio en blanco (`\\n` y `\\t` incluidos). Tambi√©n a√±adimos un cuantificador `+` al final: `\\s+`. Esto significa que queremos capturar una o m√°s ocurrencias del espacio en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d227-1149-4014-94a5-c05592a27a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se prepara una expresi√≥n regular para limpiar espacios:\n",
    "#\\s+ ‚Üí significa ‚Äúuno o m√°s espacios en blanco‚Äù (incluye tabulaciones y saltos de l√≠nea).\n",
    "blankspace_pattern = r'\\s+'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc075c2e-1a1d-4393-a3ea-8ad7c118364b",
   "metadata": {},
   "source": [
    "El reemplazo de uno o m√°s espacios en blanco es exactamente un solo espacio, que es el l√≠mite can√≥nico de palabras en ingl√©s. Cualquier espacio adicional se reducir√° a un solo espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cb2f1-f4ca-4b79-900c-f65ec303ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blankspace_repl = ' ' ‚Üí dice que cada grupo de espacios ser√° reemplazado por un solo espacio.\n",
    "blankspace_repl = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12e3d1-728a-429b-9c83-4dcc88590bc4",
   "metadata": {},
   "source": [
    "Finalmente, combinemos todo usando la funci√≥n [`re.sub()`](https://docs.python.org/3.11/library/re.html#re.sub), lo que significa que queremos sustituir un patr√≥n por un reemplazo. La funci√≥n acepta tres argumentos: el patr√≥n, el reemplazo y la cadena a la que queremos aplicar la funci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249b24b-7111-4569-be29-c40efa5e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\n"
     ]
    }
   ],
   "source": [
    "# re.sub() busca en second_example todos los espacios m√∫ltiples (\\s+) y los reemplaza por un √∫nico espacio.\n",
    "#As√≠ se normaliza el texto y queda m√°s limpio.\n",
    "clean_text = re.sub(pattern = blankspace_pattern, \n",
    "                    repl = blankspace_repl, \n",
    "                    string = second_example)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895fbe3-a034-4124-94af-72a528913c51",
   "metadata": {},
   "source": [
    "¬°Ta-da! El car√°cter de nueva l√≠nea ya no est√°."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087dc0c-5fef-4f1c-8662-7cbc8a978f34",
   "metadata": {},
   "source": [
    "### Eliminar signos de puntuaci√≥n\n",
    "\n",
    "A veces solo nos interesa analizar **caracteres alfanum√©ricos** (es decir, letras y n√∫meros), en cuyo caso podr√≠amos querer eliminar los signos de puntuaci√≥n.\n",
    "\n",
    "El m√≥dulo `string` contiene una lista de signos de puntuaci√≥n predefinidos. Vamos a imprimirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8502b-b703-45e0-8852-0c3210363440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Importa la variable punctuation desde el m√≥dulo string.\n",
    "# Esta variable contiene todos los signos de puntuaci√≥n comunes en ingl√©s\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91119c9e-431c-42cb-afea-f7e607698929",
   "metadata": {},
   "source": [
    "En la pr√°ctica, para eliminar estos caracteres de puntuaci√≥n, podemos simplemente iterar sobre el texto y eliminar los caracteres que se encuentran en la lista, como se muestra a continuaci√≥n en la funci√≥n `remove_punct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d868d-339d-4bbe-9a3b-20fa5fbdf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define una funci√≥n llamada remove_punct para eliminar signos de puntuaci√≥n en cualquier texto.\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Si el car√°cter NO est√° en la lista de punctuation, lo guarda en la lista no_punct.\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Une todos los caracteres filtrados y devuelve el texto sin puntuaci√≥n.\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc768b-c2dd-4386-8212-483c4485e4be",
   "metadata": {},
   "source": [
    "Apliquemos la funci√≥n al ejemplo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596c465-3d85-4b72-a853-f2151bcd91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica why are your first fares in May over three times more than other carriers when all seats are available to select???\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VirginAmerica why are your first fares in May over three times more than other carriers when all seats are available to select'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecciona el tweet n√∫mero 20, lo imprime y dibuja una l√≠nea separadora con \"=\" * 50\n",
    "third_example = tweets['text'][20]\n",
    "print(third_example)\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Llama a la funci√≥n remove_punct() para eliminar todos los signos de puntuaci√≥n del tweet 20.\n",
    "remove_punct(third_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a4b83-f503-4405-aedd-66bbc088e3e7",
   "metadata": {},
   "source": [
    "Vamos a intentarlo con otro tuit. ¬øQu√© has notado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c2f60-fc92-4326-bad6-5ad04be50476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica trying to add my boy Prince to my ressie. SF this Thursday @VirginAmerica from LAX http://t.co/GsB2J3c4gM\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VirginAmerica trying to add my boy Prince to my ressie SF this Thursday VirginAmerica from LAX httptcoGsB2J3c4gM'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime el tweet n√∫mero 100 del dataset.\n",
    "print(tweets['text'][100])\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# versi√≥n del tweet sin signos de puntuaci√≥n.\n",
    "remove_punct(tweets['text'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af02ce5-b674-4cb4-8e08-7d7416963f9c",
   "metadata": {},
   "source": [
    "¬øQu√© tal el siguiente ejemplo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c3947-e6b8-42fe-8a58-15e4b6c60005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weve got quite a bit of punctuation here dont we Python DLab'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqu√≠ se crea un string con: Contracciones: (\"We've\", \"don't\"); S√≠mbolos: (?!?, #, @)\n",
    "contraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n",
    "\n",
    "# elimina puntuaci√≥n tanto en datos reales (tweets) como en textos de prueba.\n",
    "remove_punct(contraction_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62574c66-db3f-4500-9c3b-cea2f3eb2a30",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Advertencia:** En muchos casos, queremos eliminar los signos de puntuaci√≥n **despu√©s** de la tokenizaci√≥n, lo cual explicaremos en breve. Esto nos indica que el **orden** del preprocesamiento es crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6b85e-58e7-4f56-9b4a-b60c85b394ba",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 1: Preprocesamiento con m√∫ltiples pasos\n",
    "\n",
    "Hasta ahora hemos aprendido algunas operaciones de preprocesamiento. ¬°Combin√©moslas en una funci√≥n! Esta funci√≥n te resultar√° √∫til si trabajas con datos de texto en ingl√©s confusos y quieres preprocesarlos con una sola funci√≥n.\n",
    "\n",
    "A continuaci√≥n se muestra el ejemplo de datos de texto para el desaf√≠o 1. Escribe una funci√≥n para:\n",
    "- Convertir el texto en min√∫sculas\n",
    "- Eliminar signos de puntuaci√≥n\n",
    "- Eliminar espacios en blanco adicionales\n",
    "\n",
    "¬°Si√©ntete libre de reciclar los c√≥digos que usamos anteriormente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb10cba-239e-4856-b56d-7d5eb850c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Abre el archivo (example1.txt), guarda el contenido en la variable challenge1.\n",
    "#Imprime en pantalla para ver c√≥mo luce antes de limpiarlo.\n",
    "challenge1_path = '../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2480823-65dd-4f52-a7b3-6d9b10d87912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "def clean_text(text):\n",
    "\n",
    "    # Convertir a min√∫sculas\n",
    "    text = ...\n",
    "\n",
    "    # Usa la funci√≥n que ya definiste antes (remove_punct) para eliminar comas, puntos, etc.\n",
    "    text = ...\n",
    "\n",
    "    # Eliminar espacios en blanco adicionales\n",
    "    text = ...\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc603506-0adb-45d7-bb6f-62958c054fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# versi√≥n limpia del archivo (example1.txt)\n",
    "# clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c159cb-8eaa-4c30-b8ff-38a712d2bb0f",
   "metadata": {},
   "source": [
    "## Procesos espec√≠ficos de cada tarea\n",
    "\n",
    "Ahora que comprendemos las operaciones comunes de preprocesamiento, a√∫n quedan algunas operaciones adicionales por considerar. Nuestros datos de texto podr√≠an requerir una mayor normalizaci√≥n seg√∫n el idioma, la fuente y el contenido de los datos.\n",
    "\n",
    "Por ejemplo, si trabajamos con documentos financieros, podr√≠amos querer estandarizar los s√≠mbolos monetarios convirti√©ndolos en d√≠gitos. En nuestros datos de tuits, existen numerosos hashtags y URL. Estos se pueden reemplazar con marcadores de posici√≥n para simplificar el an√°lisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936cea-74e9-40c2-bfbe-6ba8129330de",
   "metadata": {},
   "source": [
    "### üé¨ **Demostraci√≥n**: Eliminar Hashtags y URL\n",
    "\n",
    "Aunque las URL, los hashtags y los n√∫meros son informativos por s√≠ mismos, a menudo no nos importa su significado exacto.\n",
    "\n",
    "Si bien podr√≠amos eliminarlos por completo, suele ser informativo saber que **existe** una URL o un hashtag. En la pr√°ctica, reemplazamos las URL y los hashtags individuales con un \"s√≠mbolo\" que preserva la existencia de estas estructuras en el texto. Lo habitual es usar las cadenas \"URL\" y \"HASHTAG\".\n",
    "\n",
    "Dado que estos tipos de texto suelen seguir una estructura regular, son un buen ejemplo para usar expresiones regulares. Apliquemos estos patrones a los datos de los tuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\n"
     ]
    }
   ],
   "source": [
    "# Se obtiene el tweet n√∫mero 13 del dataset tweets y lo imprime.\n",
    "url_tweet = tweets['text'][13]\n",
    "print(url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef61bea-ea11-468d-8176-a2f63659d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel  URL \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (url_pattern) es una expresi√≥n regular que detecta enlaces.\n",
    "#(http|ftp|https):\\/\\/ ‚Üí busca enlaces que empiecen con http, https o ftp.\n",
    "#([\\w_-]+(?:(?:\\.[\\w_-]+)+)) ‚Üí busca dominios (ej. twitter.com)\n",
    "# ([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-]) ‚Üí busca la parte extra despu√©s del dominio (par√°metros, rutas, etc).\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "url_repl = ' URL '\n",
    "# re.sub(...) reemplaza todo el enlace completo con la palabra \" URL \".\n",
    "re.sub(url_pattern, url_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your HASHTAG  HASHTAG  skies again! U take all the HASHTAG  away from travel http://t.co/ahlXHhKiyn\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reemplazar hashtags por \"HASHTAG\".\n",
    "hashtag_pattern = r'(?:^|\\s)[ÔºÉ#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d68d49-4923-49c0-9113-b844dc7546b9",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Tokenizaci√≥n\n",
    "\n",
    "## Tokenizadores antes de los LLM\n",
    "\n",
    "Uno de los pasos m√°s importantes en el an√°lisis de texto es la tokenizaci√≥n. Este proceso consiste en descomponer una secuencia larga de texto en tokens de palabras. Con estos tokens disponibles, estamos listos para realizar un an√°lisis a nivel de palabra. Por ejemplo, podemos filtrar los tokens que no contribuyen al significado central del texto.\n",
    "\n",
    "En esta secci√≥n, presentaremos c√≥mo realizar la tokenizaci√≥n utilizando `nltk`, `spaCy` y un Modelo de Lenguaje Grande (`bert`). El objetivo es presentarle diferentes paquetes de PLN, ayudarle a comprender sus funcionalidades y mostrarle c√≥mo acceder a las funciones clave de cada paquete.\n",
    "\n",
    "### `nltk`\n",
    "\n",
    "El primer paquete que usaremos se llama **Natural Language Toolkit**, o `nltk`.\n",
    "\n",
    "Instalemos un par de m√≥dulos del paquete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441d81f8-361e-4273-bd36-91a272f4a38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b327cc-5c77-4fdc-9aaf-17d7f0761237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet ‚Üí recurso l√©xico en ingl√©s (se usa para sin√≥nimos, lematizaci√≥n, etc).\n",
    "# nltk.download('wordnet')\n",
    "#stopwords ‚Üí lista de palabras vac√≠as (ej: \"the\", \"and\", \"is\", que no aportan mucho significado).\n",
    "# nltk.download('stopwords')\n",
    "#punkt ‚Üí modelo de tokenizaci√≥n pre-entrenado (sirve para cortar oraciones y palabras).\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79b699-c3a5-489f-9b3c-95653aba34d6",
   "metadata": {},
   "source": [
    "`nltk` tiene una funci√≥n llamada `word_tokenize`. Requiere un argumento, que es el texto que se tokenizar√°, y nos devuelve una lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6944-c641-4fac-a239-5947a496371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n"
     ]
    }
   ],
   "source": [
    "# Cargar word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Toma el tweet n√∫mero 7 del dataset.\n",
    "text = tweets['text'][7]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fde2a3-e4e2-4e61-ad54-e4d5d0a6ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize(text) divide el tweet en tokens (palabras, n√∫meros, signos de puntuaci√≥n, etc).\n",
    "nltk_tokens = word_tokenize(text)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ead039-7721-4b22-8590-0d7824631675",
   "metadata": {},
   "source": [
    "Aqu√≠ tenemos una lista de tokens identificados por `nltk`. ¬°Revis√©moslos un momento!\n",
    "\n",
    "üîî **Pregunta**: ¬øTienes sentido para ti los l√≠mites de palabras definidos por `nltk`? Presta atenci√≥n al usuario de Twitter y a la URL del tuit de ejemplo.\n",
    "\n",
    "Puede que te parezca que acceder a las funciones de `nltk` es bastante sencillo. La funci√≥n que usamos anteriormente se import√≥ del m√≥dulo `nltk.tokenize`, que, como su nombre indica, se encarga principalmente de la tokenizaci√≥n.\n",
    "\n",
    "En esencia, `nltk` cuenta con [una colecci√≥n de m√≥dulos](https://www.nltk.org/api/nltk.html) que cumplen diferentes funciones, por nombrar algunas:\n",
    "\n",
    "| m√≥dulo NLTK   | Funci√≥n                | Enlace                                                         |\n",
    "|---------------|---------------------------|--------------------------------------------------------------|\n",
    "| nltk.tokenize | Tokenizaci√≥n              | [Documentaci√≥n](https://www.nltk.org/api/nltk.tokenize.html) |\n",
    "| nltk.corpus   | Recuperar corpus integrados | [Documentaci√≥n](https://www.nltk.org/nltk_data/)             |\n",
    "| nltk.tag      | Etiquetado de partes del discurso    | [Documentaci√≥n](https://www.nltk.org/api/nltk.tag.html)      |\n",
    "| nltk.stem     | Derivado                  | [Documentaci√≥n](https://www.nltk.org/api/nltk.stem.html)     |\n",
    "| ...           | ...                       | ...                                                          |\n",
    "\n",
    "Importemos `stopwords` desde el m√≥dulo `nltk.corpus`, que alberga una variedad de corpus integrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbfced-8803-41ca-9cae-49bdadf8c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee971a1-1189-4cb6-8317-4836f54c3ae2",
   "metadata": {},
   "source": [
    "Especifiquemos que queremos recuperar palabras vac√≠as en ingl√©s. La funci√≥n simplemente devuelve una lista de palabras vac√≠as, principalmente palabras de funci√≥n, que `nltk` identifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e1df-b720-4d22-a162-7fd250a58672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Carga la lista de palabras vac√≠as en ingl√©s.\n",
    "#El [:10] solo imprime las primeras 10.\n",
    "stop = stopwords.words('english')\n",
    "stop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec908-de6c-42c5-a370-f1b1df0032b3",
   "metadata": {},
   "source": [
    "### `spaCy`\n",
    "Adem√°s de `nltk`, contamos con otro paquete muy utilizado llamado `spaCy`. \n",
    "\n",
    "`spaCy` cuenta con su propia canalizaci√≥n de procesamiento. Recibe una cadena de texto, ejecuta la canalizaci√≥n `nlp` en ella y almacena el texto procesado y sus anotaciones en un objeto llamado `doc`. La canalizaci√≥n `nlp` siempre realiza la tokenizaci√≥n, as√≠ como otros componentes de an√°lisis de texto solicitados por el usuario. Estos componentes son bastante similares a los m√≥dulos de `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0facd-4b75-41ac-920c-5ea044f7ae2e",
   "metadata": {},
   "source": [
    "<img src='../images/spacy.png' alt=\"spacy pipeline\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef1eaf-2790-4928-b094-943f2803c6a0",
   "metadata": {},
   "source": [
    "Tenga en cuenta que siempre comenzamos inicializando la secuencia de comandos ¬´nlp¬ª, seg√∫n el idioma del texto. Aqu√≠, cargamos un modelo de idioma preentrenado para ingl√©s: ¬´en_core_web_sm¬ª. El nombre sugiere que se trata de un modelo ligero entrenado con datos de texto (por ejemplo, blogs); consulte las descripciones de los modelos [aqu√≠](https://spacy.io/models/en#en_core_web_sm).\n",
    "\n",
    "Esta es la primera vez que nos encontramos con el concepto de preentrenamiento, aunque quiz√°s ya lo haya o√≠do en otros lugares. En el contexto del PLN, el preentrenamiento significa que el modelo se ha entrenado con una gran cantidad de datos. Como resultado, incorpora un cierto conocimiento de la estructura de las palabras y la gram√°tica del idioma.\n",
    "\n",
    "Por lo tanto, al aplicar el modelo a nuestros propios datos, podemos esperar que sea razonablemente preciso al realizar diversas tareas de anotaci√≥n, por ejemplo, etiquetar la categor√≠a gramatical de una palabra, identificar el n√∫cleo sint√°ctico de una frase, etc.\n",
    "\n",
    "¬°Comencemos! Primero, debemos cargar el modelo de lenguaje preentrenado que instalamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "524dfc02-aa8f-4888-9f81-74a570db72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d669c3-2f5a-41b6-893b-ea1d438b3a48",
   "metadata": {},
   "source": [
    "La canalizaci√≥n `nlp` incluye, por defecto, un conjunto de componentes a los que podemos acceder mediante el atributo `.pipe_names`.\n",
    "\n",
    "Puede que notes que no incluye el tokenizador. ¬°No te preocupes! El tokenizador es un componente especial que la canalizaci√≥n siempre incluye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d581ca5-43f8-4ef9-b099-2fc92c324581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve components included in NLP pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e37f91-d174-4101-bfc6-2859cb0fe5cc",
   "metadata": {},
   "source": [
    "Ejecutemos el pipeline `nlp` en nuestros datos de tweet de ejemplo y asign√©moslo a una variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "618e8558-625d-4546-8109-63f9bae9790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to example tweet\n",
    "doc = nlp(tweets['text'][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54325d60-5c5c-488d-baf2-7eed4de2c031",
   "metadata": {},
   "source": [
    "En esencia, el objeto `doc` contiene los tokens (creados por el tokenizador) y sus anotaciones (creadas por otros componentes), que son caracter√≠sticas ling√º√≠sticas (https://spacy.io/usage/linguistic-features) √∫tiles para el an√°lisis de texto. Recuperamos el token y sus anotaciones accediendo a los atributos correspondientes. \n",
    "\n",
    "| Atributo     | Anotaci√≥n                              | Enlace                                                                      |\n",
    "|----------------|-----------------------------------------|---------------------------------------------------------------------------|\n",
    "| token.text     | El token en texto textual              | [Documentation](https://spacy.io/api/token#attributes)                    |\n",
    "| token.is_stop  | Si el token es una stop word        | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.is_punct | Si el token es un signo de puntuaci√≥n | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.lemma_   | La forma base del token             | [Documentation](https://spacy.io/usage/linguistic-features#lemmatization) |\n",
    "| token.pos_     | La etiqueta POS simple del token        | [Documentation](https://spacy.io/usage/linguistic-features#pos-tagging)   |\n",
    "| ...            | ...                                     | ...                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f23c8-a157-44a7-a6ec-6894aec1a595",
   "metadata": {},
   "source": [
    "Primero, obtengamos los tokens. Iteraremos sobre el objeto `doc` y recuperaremos el texto de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c71efee-e6cf-46c4-9198-593304f6560d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https://t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the verbatim texts of tokens\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4fc23f0-c699-45e6-ad62-e131036d601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the NLTK tokens\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ace59e-40e0-42b3-9f2b-d30ac94dccab",
   "metadata": {},
   "source": [
    "üîî **Pregunta**: Deteng√°monos un momento para comparar los tokens generados por `nltk` y `spaCy`. ¬øQu√© han observado?\n",
    "\n",
    "Recuerden que tambi√©n podemos acceder a varias anotaciones de estos tokens. Por ejemplo, una anotaci√≥n que ofrece `spaCy` es que codifica f√°cilmente si un token es una palabra vac√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "626af687-e986-4c97-af86-edf7dbd22c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the is_stop annotation\n",
    "spacy_stops = [token.is_stop for token in doc]\n",
    "\n",
    "# The results are boolean values\n",
    "spacy_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6548b6-7e89-4f42-b8cb-bf7c93b34eb4",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 2: Eliminar palabras vac√≠as\n",
    "\n",
    "Ya conocemos el funcionamiento de `nltk` y `spaCy` como paquetes de PLN. Tambi√©n hemos demostrado c√≥mo identificar palabras vac√≠as con cada paquete.\n",
    "\n",
    "Escribamos **dos** funciones para eliminar palabras vac√≠as de nuestros datos de texto.\n",
    "\n",
    "- Completar la funci√≥n para eliminar palabras vac√≠as usando `nltk`\n",
    "- El c√≥digo inicial requiere dos argumentos: la entrada de texto sin formato y una lista de palabras vac√≠as predefinidas.\n",
    "- Completar la funci√≥n para eliminar palabras vac√≠as usando `spaCy`\n",
    "- El c√≥digo inicial requiere un argumento: la entrada de texto sin formato.\n",
    "\n",
    "Un recordatorio antes de empezar: ambas funciones toman texto sin formato como entrada; ¬°Eso es una se√±al para realizar primero la tokenizaci√≥n del texto sin formato!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b5370-392f-420d-8c9e-78146d0fca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    pass\n",
    "    \n",
    "    # Step 1: Tokenization with nltk\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5bbda-0af5-49a9-8f5e-77d61ab217e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_spacy(raw_text):\n",
    "    pass\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 2: Filter out tokens that are stop words\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3b0da-2223-4d7f-9014-696498e804e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83538ba-6bf1-49ca-90ec-b6532b1ffcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b1ec-87cc-4a08-a5dd-0210a9c56f0b",
   "metadata": {},
   "source": [
    "## üé¨ **Demostraci√≥n**: Potentes funciones de `spaCy`\n",
    "\n",
    "El pipeline de procesamiento de lenguaje natural (PLN) de `spaCy` incluye diversas anotaciones ling√º√≠sticas que pueden resultar muy √∫tiles para el an√°lisis de texto.\n",
    "\n",
    "Por ejemplo, podemos acceder a m√°s anotaciones, como el lema, la etiqueta gramatical y su significado, y si el token se asemeja a una URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb6c7d93-51a3-4fb8-8321-cb672f4f1b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica           | @VirginAmerica           | PROPN        | proper noun  | 0            |\n",
      "Really                   | really                   | ADV          | adverb       | 0            |\n",
      "missed                   | miss                     | VERB         | verb         | 0            |\n",
      "a                        | a                        | DET          | determiner   | 0            |\n",
      "prime                    | prime                    | ADJ          | adjective    | 0            |\n",
      "opportunity              | opportunity              | NOUN         | noun         | 0            |\n",
      "for                      | for                      | ADP          | adposition   | 0            |\n",
      "Men                      | Men                      | PROPN        | proper noun  | 0            |\n",
      "Without                  | without                  | ADP          | adposition   | 0            |\n",
      "Hats                     | Hats                     | PROPN        | proper noun  | 0            |\n",
      "parody                   | parody                   | NOUN         | noun         | 0            |\n",
      ",                        | ,                        | PUNCT        | punctuation  | 0            |\n",
      "there                    | there                    | ADV          | adverb       | 0            |\n",
      ".                        | .                        | PUNCT        | punctuation  | 0            |\n",
      "https://t.co/mWpG7grEZP  | https://t.co/mWpG7grEZP  | PROPN        | proper noun  | 1            |\n"
     ]
    }
   ],
   "source": [
    "# Print tokens and their annotations\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<24} | {token.lemma_:<24} | {token.pos_:<12} | {spacy.explain(token.pos_):<12} | {token.like_url:<12} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17388e0c-88b6-4cd9-8d2b-adb7f10b5330",
   "metadata": {},
   "source": [
    "Como puedes imaginar, es habitual que este conjunto de datos contenga nombres de lugares y c√≥digos de aeropuertos. Ser√≠a genial si pudi√©ramos identificarlos y extraerlos de los tuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb489f78-fbb2-497b-a36d-3400c00c9b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JetBlue Vegas, San Francisco, Baltimore, San Diego and Philadelphia so far! I'm a very frequent business traveler.\n",
      "==================================================\n",
      "@VirginAmerica Flying LAX to SFO and after looking at the awesome movie lineup I actually wish I was on a long haul.\n"
     ]
    }
   ],
   "source": [
    "# Print example tweets with place names and airport codes\n",
    "tweet_city = tweets['text'][8273]\n",
    "tweet_airport = tweets['text'][502]\n",
    "print(tweet_city)\n",
    "print(f\"{'=' * 50}\")\n",
    "print(tweet_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013990a5-5e07-4a45-9427-fcd33840d3b8",
   "metadata": {},
   "source": [
    "Podemos utilizar el componente \"ner\" ([Reconocimiento de entidades nombradas](https://en.wikipedia.org/wiki/Named-entity_recognition)) para identificar entidades y sus categor√≠as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9e63519-5991-49fa-9a5d-be9f9b408ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegas           | 9          | 14         | GPE       \n",
      "San Francisco   | 16         | 29         | GPE       \n",
      "Baltimore       | 31         | 40         | GPE       \n",
      "San Diego       | 42         | 51         | GPE       \n",
      "Philadelphia    | 56         | 68         | GPE       \n"
     ]
    }
   ],
   "source": [
    "# Print entities identified from the text\n",
    "doc_city = nlp(tweet_city)\n",
    "for ent in doc_city.ents:\n",
    "    print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b933ed0-7018-450c-b0a6-fb76cb6d5be9",
   "metadata": {},
   "source": [
    "Tambi√©n podemos usar ¬´desplazamiento¬ª para resaltar las entidades identificadas en el texto y, al mismo tiempo, anotar la categor√≠a de la entidad.\n",
    "\n",
    "En el siguiente ejemplo, tenemos cuatro ¬´GPE¬ª (es decir, entidades geopol√≠ticas, generalmente pa√≠ses y ciudades) identificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a5a5219-af1f-445c-a35b-7c49d739a91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@JetBlue \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vegas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Baltimore\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Diego\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Philadelphia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " so far! I'm a very frequent business traveler.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the identified entities\n",
    "from spacy import displacy\n",
    "displacy.render(doc_city, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7953b-04a1-46d0-817a-db29edd8c83b",
   "metadata": {},
   "source": [
    "Vamos a intentarlo con otro ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9246c7e3-8990-4d47-a355-deb63dbd1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica  | 0          | 14         | CARDINAL  \n",
      "Flying LAX      | 15         | 25         | ORG       \n",
      "SFO             | 29         | 32         | ORG       \n"
     ]
    }
   ],
   "source": [
    "# Print entities identified from the text\n",
    "doc_airport = nlp(tweet_airport)\n",
    "for ent in doc_airport.ents:\n",
    "     print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df38472-3193-44c5-9b2f-e311ce9d42e0",
   "metadata": {},
   "source": [
    "Es interesante que los c√≥digos de los aeropuertos est√©n identificados como ¬´ORG¬ª (organizaciones) y el identificador del tweet como ¬´CARDINAL¬ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e4bf382-4c57-4b78-a37f-fc1f6cb1d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    @VirginAmerica\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Flying LAX\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SFO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and after looking at the awesome movie lineup I actually wish I was on a long haul.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the identified entities\n",
    "displacy.render(doc_airport, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d6f28-effc-4fe1-90e9-47c89dc5492d",
   "metadata": {},
   "source": [
    "## Tokenizadores desde los LLM\n",
    "\n",
    "Hasta ahora, hemos visto c√≥mo funciona la tokenizaci√≥n con dos paquetes de PLN ampliamente utilizados. Funcionan bastante bien en algunos entornos, pero no en otros. Recordemos que `nltk` tiene dificultades con las URL. Ahora, imaginemos que los datos que tenemos son a√∫n m√°s confusos, con errores ortogr√°ficos, palabras de reciente creaci√≥n, nombres extranjeros, etc. (denominados colectivamente palabras \"fuera de vocabulario\" u OOV). En tales circunstancias, podr√≠amos necesitar un modelo m√°s potente para gestionar estas complejidades.\n",
    "\n",
    "De hecho, los esquemas de tokenizaci√≥n cambian sustancialmente con los **Grandes Modelos de Lenguaje** (LLM), que son modelos entrenados con una enorme cantidad de datos de fuentes mixtas. Con esa magnitud de datos, los LLM son m√°s eficaces para fragmentar una secuencia m√°s larga en tokens y estos en **subtokens**. Estos subtokens pueden ser unidades morfol√≥gicas de una palabra, como un afijo, pero tambi√©n pueden ser partes de una palabra donde el modelo establece un l√≠mite \"significativo\". En esta secci√≥n, demostraremos la tokenizaci√≥n en **BERT** (Representaciones de Codificador Bidireccional de Transformers), que utiliza un algoritmo de tokenizaci√≥n llamado [**WordPiece**](https://huggingface.co/learn/nlp-course/en/chapter6/6).\n",
    "\n",
    "Cargaremos el tokenizador de BERT desde el paquete `transformers`, que aloja varios LLM basados ‚Äã‚Äãen Transformers (por ejemplo, BERT). No profundizaremos en la arquitectura de Transformer en este taller, pero pueden consultar el taller de D-lab sobre [Fundamentos de GPT](https://github.com/dlab-berkeley/GPT-Fundamentals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1509b-5b9e-456d-909f-a6b5099c48c8",
   "metadata": {},
   "source": [
    "Tokenizaci√≥n de WordPiece\n",
    "\n",
    "Tenga en cuenta que BERT est√° disponible en varias versiones. La que exploraremos hoy es `bert-base-uncased`. Este modelo tiene un tama√±o moderado (denominado `base`) y no distingue entre may√∫sculas y min√∫sculas, lo que significa que el texto de entrada se escribir√° en min√∫sculas por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927226c-d04e-4117-9c49-3d355611b209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfb38-274e-4d75-9d14-8744020fe49c",
   "metadata": {},
   "source": [
    "El tokenizador tiene m√∫ltiples funciones, como veremos en un minuto. Ahora queremos acceder a la funci√≥n `.tokenize()` desde el tokenizador.\n",
    "\n",
    "Tokenicemos un tweet de ejemplo a continuaci√≥n. ¬øQu√© has notado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62649193-bb00-4ae8-9102-bce3d1dfb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @VirginAmerica Just DM'd. Same issue persisting.\n",
      "==================================================\n",
      "Tokens: ['@', 'virgin', '##ame', '##rica', 'just', 'd', '##m', \"'\", 'd', '.', 'same', 'issue', 'persist', '##ing', '.']\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "# Select an example tweet from dataframe\n",
    "text = tweets['text'][194]\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f780e-207c-4d3d-b1b6-063c6d118945",
   "metadata": {},
   "source": [
    "Los s√≠mbolos de doble \"hashtag\" (`##`) hacen referencia a un token de subpalabra: un segmento separado del token anterior.\n",
    "\n",
    "üîî **Pregunta**: ¬øTienen sentido estas subpalabras?\n",
    "\n",
    "Un avance significativo de los LLM es que a cada token se le asigna un ID de su vocabulario. Nuestra computadora no entiende el texto en su forma original, por lo que cada token se traduce en un ID. Estos ID son las entradas a las que el modelo accede y con las que opera.\n",
    "\n",
    "Los tokens y los ID se pueden convertir bidireccionalmente, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3112a63d-82b6-4fc0-a904-ab86f8740653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of just is: 2074\n",
      "Token 2074 is: just\n"
     ]
    }
   ],
   "source": [
    "# Get the input ID of the word \n",
    "print(f\"ID of just is: {tokenizer.vocab['just']}\")\n",
    "\n",
    "# Get the text of the input ID\n",
    "print(f\"Token 2074 is: {tokenizer.decode([2074])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fd13d-f26e-480c-b43e-2b5fbc4898cd",
   "metadata": {},
   "source": [
    "Convirtamos los tokens en identificaciones de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d125e1d-2560-4136-829c-b1c11e34636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input IDs: 15\n",
      "Input IDs of text: [1030, 6261, 14074, 14735, 2074, 1040, 2213, 1005, 1040, 1012, 2168, 3277, 29486, 2075, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Convert a list of tokens to a list of input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Number of input IDs: {len(input_ids)}\")\n",
    "print(f\"Input IDs of text: {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a1a14-a6db-414b-a1a8-c6be73c81a15",
   "metadata": {},
   "source": [
    "### Tokens Especiales\n",
    "\n",
    "Adem√°s de los tokens y subtokens mencionados anteriormente, BERT tambi√©n utiliza tres tokens especiales: `SEP`, `CLS` y `UNK`. El token `SEP` act√∫a como terminador de oraci√≥n, com√∫nmente conocido como token `EOS` (Fin de Oraci√≥n). El token `UNK` representa cualquier token que no se encuentre en el vocabulario, de ah√≠ los tokens \"desconocidos\". El token `CLS` se a√±ade al principio de la oraci√≥n. Su origen se remonta a tareas de clasificaci√≥n de texto (p. ej., detecci√≥n de spam), donde los investigadores encontraron √∫til un token que agregara la informaci√≥n de toda la oraci√≥n para fines de clasificaci√≥n.\n",
    "\n",
    "Cuando aplicamos `tokenizer()` directamente a nuestros datos de texto, le pedimos a BERT que **codifique** el texto. Esto implica varios pasos:\n",
    "- Tokenizar el texto\n",
    "- A√±adir tokens especiales\n",
    "- Convertir tokens en ID de entrada\n",
    "- Otros procesos espec√≠ficos del modelo\n",
    "\n",
    "Vamos a imprimirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21479c25-7a9a-4fac-ba09-1812575b8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input IDs: 17\n",
      "IDs from tokenizer: [101, 1030, 6261, 14074, 14735, 2074, 1040, 2213, 1005, 1040, 1012, 2168, 3277, 29486, 2075, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Get the input IDs by providing the key \n",
    "input_ids_from_tokenizer = tokenizer(text)['input_ids']\n",
    "print(f\"Number of input IDs: {len(input_ids_from_tokenizer)}\")\n",
    "print(f\"IDs from tokenizer: {input_ids_from_tokenizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ea98e-7374-488a-a804-46cab166125c",
   "metadata": {},
   "source": [
    "Parece que hemos a√±adido dos tokens m√°s: 101 y 102.\n",
    "\n",
    "¬°Vamos a convertirlos en texto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82a18dc1-ca0d-4d5b-8ac6-f56cb44bf8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 101st token: [CLS]\n",
      "The 102nd token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert input IDs to texts\n",
    "print(f\"The 101st token: {tokenizer.convert_ids_to_tokens(101)}\")\n",
    "print(f\"The 102nd token: {tokenizer.convert_ids_to_tokens(102)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d4e10-e96c-432a-a6c9-2c992d00fcde",
   "metadata": {},
   "source": [
    "Como puede ver, nuestro ejemplo de texto ahora es una lista de identificadores de vocabulario. Adem√°s, BERT a√±ade el terminador de oraci√≥n ¬´SEP¬ª y el token inicial ¬´CLS¬ª al texto original. El tokenizador de BERT tambi√©n codifica una gran cantidad de textos; posteriormente, est√°n listos para su posterior procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56be32-2a5f-441e-8283-de3e60705c0b",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 3: Encuentra el l√≠mite de palabra\n",
    "\n",
    "Ahora que sabemos que la tokenizaci√≥n en BERT suele generar subpalabras, probemos con algunos ejemplos m√°s.\n",
    "\n",
    "- ¬øCu√°l crees que es el l√≠mite correcto para dividir las siguientes palabras en subpalabras?\n",
    "- ¬øQu√© otros ejemplos has probado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1140b19-398c-44dd-829c-c922b0e6f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c07c71e-5be2-4d91-9c6f-26de4145307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl', '##ab']\n",
      "['co', '##vid']\n",
      "['hug', '##ga', '##ble']\n",
      "['37', '##8']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb7cb9-6e60-4e0a-8cc6-21d57237e835",
   "metadata": {},
   "source": [
    "Concluiremos la Parte 1 con este desaf√≠o (que esperamos) que invita a la reflexi√≥n. Los LLM suelen incluir un esquema de tokenizaci√≥n mucho m√°s sofisticado, pero existe un debate continuo sobre sus limitaciones en aplicaciones pr√°cticas. La secci√≥n de referencia incluye algunas entradas de blog que abordan este problema. ¬°No dudes en explorar m√°s a fondo si esta pregunta te parece interesante!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7943ed9-70de-4f4a-b1bb-b2896d05e618",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "1. Un tutorial que presenta el esquema de tokenizaci√≥n en BERT: [El curso de PNL de huggingface sobre tokenizaci√≥n de piezas de palabra](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n",
    "2. Un ejemplo espec√≠fico de \"fracaso\" en la tokenizaci√≥n: [Debilidades de la tokenizaci√≥n de piezas de palabra: Hallazgos desde la primera l√≠nea del PNL en VMware](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99)\n",
    "3. ¬øC√≥mo determina BERT los l√≠mites entre subtokens? [Tokenizaci√≥n de subpalabras en BERT](https://tinkerd.net/blog/machine-learning/bert-tokenization/#subword-tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0812a7-f033-46ed-bc7b-67109c369e6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Puntos clave\n",
    "\n",
    "* El preprocesamiento incluye varios pasos, algunos m√°s comunes para los datos de texto y otros espec√≠ficos de cada tarea.\n",
    "* Tanto `nltk` como `spaCy` pueden utilizarse para la tokenizaci√≥n y la eliminaci√≥n de palabras vac√≠as. Este √∫ltimo es m√°s potente a la hora de proporcionar diversas anotaciones ling√º√≠sticas.\n",
    "* La tokenizaci√≥n funciona de forma diferente en BERT, que a menudo implica la descomposici√≥n de una palabra completa en subpalabras.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
